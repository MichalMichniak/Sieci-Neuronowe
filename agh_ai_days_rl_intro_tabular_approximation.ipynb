{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalMichniak/Sieci-Neuronowe/blob/main/agh_ai_days_rl_intro_tabular_approximation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabular approximation with Tic-Tac-Toe\n",
        "\n",
        "Now, let's deal with something slightly more advanced:\n",
        "a game of multiple states and discrete actions: Tic-Tac-Toe.\n",
        "\n",
        "Let's try to solve this problem!\n",
        "\n",
        "<img src=\"https://mk.ssb-media.com/images/alt_368741_1_2x.jpg\" alt=\"drawing\" width=\"400px\"/>\n",
        "\n",
        "## Credits\n",
        "- Interactive Tic-Tac-Toe example has been taken from [this Jupyter Notebook](https://colab.research.google.com/github/asolin/notebooks/blob/main/notebooks/RL-for-Tic-Tac-Toe.ipynb) by [Arno Solin](https://github.com/asolin/).\n",
        "- Maciej Aleksandrowicz for AGH AI Days 2024\n",
        "\n",
        "### Further resources\n",
        "- Another source for Tabular Q-Learning for Tic-Tac-Toe can be find in [this Github repository](https://github.com/sunbri/tictactoe) by [Brian Sun](https://github.com/sunbri)."
      ],
      "metadata": {
        "id": "exgevrXmGseU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Tic-Tac-Toe\n",
        "\n",
        "First, let's import an example of interactive game into this notebook. Play with it for a while."
      ],
      "metadata": {
        "id": "pxVjL-vKKOkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "rGkkn_v2_bOS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code for interactive game\n",
        " * Run cell and hide this section with the environement defition.\n",
        " * Note, that in this `O` and `X` notation, the `X`s always starts."
      ],
      "metadata": {
        "id": "YDGgzAzfLBpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "   Copyright 2017 Neil Slater\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "\n",
        "   The original code as been modified for educational purposes.\n",
        "   Further modifications for education were made in 2024 for AGH AI Days.\n",
        "'''\n",
        "\n",
        "class TicTacToeGame():\n",
        "    def __init__(self):\n",
        "        self.state = \"         \"  #a string of length 9 that encode the state of the 3*3 board\n",
        "        self.player = \"X\"\n",
        "        self.winner = None\n",
        "\n",
        "    def allowed_moves(self):      #find the empty position in the state string\n",
        "        states = []               #store all possible next states\n",
        "        for i in range(len(self.state)):\n",
        "            if self.state[i] == ' ':\n",
        "                states.append(self.state[:i] + self.player + self.state[i+1:])\n",
        "        return states\n",
        "\n",
        "    def make_move(self, next_state):\n",
        "        if self.winner:\n",
        "            raise(Exception(\"Game already completed, cannot make another move!\"))\n",
        "        if not self.__valid_move(next_state):\n",
        "            raise(Exception(\"Cannot make move {} to {} for player {}\".format(\n",
        "                    self.state, next_state, self.player)))\n",
        "\n",
        "        self.state = next_state\n",
        "        self.winner = self.predict_winner(self.state)\n",
        "        if self.winner:\n",
        "            self.player = None\n",
        "        elif self.player == 'X':\n",
        "            self.player = 'O'\n",
        "        else:\n",
        "            self.player = 'X'\n",
        "\n",
        "    def playable(self):\n",
        "        return ( (not self.winner) and any(self.allowed_moves()) )\n",
        "\n",
        "    def predict_winner(self, state):\n",
        "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]  # all possible lines\n",
        "        winner = None\n",
        "        for line in lines:\n",
        "            line_state = state[line[0]] + state[line[1]] + state[line[2]]\n",
        "            if line_state == 'XXX':\n",
        "                winner = 'X'\n",
        "            elif line_state == 'OOO':\n",
        "                winner = 'O'\n",
        "        return winner\n",
        "\n",
        "    def __valid_move(self, next_state):\n",
        "        allowed_moves = self.allowed_moves()  #get all possible next states\n",
        "        if any(state == next_state for state in allowed_moves): #check if the input next_state is in\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def print_help(self):\n",
        "        s = tuple([x + 1 for x in range(9)])\n",
        "        self._print_state(s)\n",
        "\n",
        "    def print_board(self):\n",
        "        s = self.state\n",
        "        self._print_state(s)\n",
        "\n",
        "    def _print_state(self, s):\n",
        "        print(f\"  {s[0]} | {s[1]} | {s[2]} \")\n",
        "        print(f\" -----------\")\n",
        "        print(f\"  {s[3]} | {s[4]} | {s[5]} \")\n",
        "        print(f\" -----------\")\n",
        "        print(f\"  {s[6]} | {s[7]} | {s[8]} \")\n"
      ],
      "metadata": {
        "id": "1Zf2Wm7hKnIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code for an agent\n",
        " * Inspect code for all TODOs to resolve.\n",
        " * Run cell and hide this section with the agent defition - we will take about it more later.\n"
      ],
      "metadata": {
        "id": "k9bh_uMRMYjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, game_class, epsilon=0.1, alpha=0.5, value_player='X'):\\\n",
        "        # -------------------------------------------------------------------- #\n",
        "        # TODO: initalize an empty dict (our table for values)\n",
        "        self.V = None\n",
        "        self.V = dict()\n",
        "        # -------------------------------------------------------------------- #\n",
        "        self.NewGame = game_class\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.value_player = value_player\n",
        "\n",
        "    def state_value(self, game_state: str) -> float:\n",
        "        # -------------------------------------------------------------------- #\n",
        "        # TODO: get value of a state, or return 0.0\n",
        "        # self.V.get(...)\n",
        "        # return 0.0\n",
        "\n",
        "        # -------------------------------------------------------------------- #\n",
        "\n",
        "    def learn_game(self, num_episodes=1000):\n",
        "        for episode in range(num_episodes):\n",
        "            self.learn_from_episode()\n",
        "\n",
        "    def learn_from_episode(self):\n",
        "        game = self.NewGame()\n",
        "        _, move = self.learn_select_move(game)\n",
        "        while move:\n",
        "            move = self.learn_from_move(game, move)\n",
        "\n",
        "    def learn_from_move(self, game, move):\n",
        "        game.make_move(move)\n",
        "        r = self.__reward(game)\n",
        "        td_target = r\n",
        "        next_state_value = 0.0\n",
        "        selected_next_move = None\n",
        "        if game.playable():\n",
        "            best_next_move, selected_next_move = self.learn_select_move(game)\n",
        "            next_state_value = self.state_value(best_next_move)\n",
        "        current_state_value = self.state_value(move)\n",
        "        td_target = r + next_state_value\n",
        "\n",
        "        # -------------------------------------------------------------------- #\n",
        "        # TODO: Implement update equation: V(a) = V(a) + alpha * (TARGET - V(a))\n",
        "        # where:\n",
        "        # a => move\n",
        "        # V => self.V\n",
        "        # alpha => self.alpha\n",
        "        # target => td_target\n",
        "        # reading V value => current_state_value\n",
        "        # ----\n",
        "        # self.V[...] = ...\n",
        "\n",
        "        # -------------------------------------------------------------------- #\n",
        "        return selected_next_move\n",
        "\n",
        "    def learn_select_move(self, game):\n",
        "        allowed_state_values = self.__state_values( game.allowed_moves() )\n",
        "        if game.player == self.value_player:\n",
        "            best_move = self.__argmax_V(allowed_state_values)\n",
        "        else:\n",
        "            best_move = self.__argmin_V(allowed_state_values)\n",
        "\n",
        "        selected_move = best_move\n",
        "        if random.random() < self.epsilon:  #epsilon-greedy strategy\n",
        "            selected_move = self.__random_V(allowed_state_values)\n",
        "\n",
        "        return (best_move, selected_move)\n",
        "\n",
        "    def play_select_move(self, game):\n",
        "        allowed_state_values = self.__state_values( game.allowed_moves() )\n",
        "        if game.player == self.value_player:\n",
        "            return self.__argmax_V(allowed_state_values)\n",
        "        else:\n",
        "            #return self.__argmin_V(allowed_state_values)\n",
        "            return self.__random_V(allowed_state_values)\n",
        "\n",
        "    def demo_game(self, verbose=False):\n",
        "        game = self.NewGame()\n",
        "        t = 0\n",
        "        while game.playable():\n",
        "            if verbose:\n",
        "                print(f\" \\nTurn {t}\\n\")\n",
        "                game.print_board()\n",
        "            move = self.play_select_move(game)\n",
        "            game.make_move(move)\n",
        "            t += 1\n",
        "        if verbose:\n",
        "            print(\" \\nTurn {}\\n\".format(t))\n",
        "            game.print_board()\n",
        "        if game.winner:\n",
        "            if verbose:\n",
        "                print(\"\\n{} is the winner!\".format(game.winner))\n",
        "            return game.winner\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"\\nIt's a draw!\")\n",
        "            return '-'\n",
        "\n",
        "    def interactive_game(self, agent_player='X'):\n",
        "        game = self.NewGame()\n",
        "        t = 0\n",
        "        while game.playable():\n",
        "          if t == 0:\n",
        "            print(\" \\nBOARD PLACES\\n\")\n",
        "            game.print_help()\n",
        "          print(\" \\nTurn {}\\n\".format(t))\n",
        "          game.print_board()\n",
        "          if game.player == agent_player:\n",
        "              move = self.play_select_move(game)\n",
        "              game.make_move(move)\n",
        "          else:\n",
        "              move = self.__request_human_move(game)\n",
        "              game.make_move(move)\n",
        "          t += 1\n",
        "\n",
        "        print(\" \\nTurn {}\\n\".format(t))\n",
        "        game.print_board()\n",
        "\n",
        "        if game.winner:\n",
        "            print(\"\\n{} is the winner!\".format(game.winner))\n",
        "            return game.winner\n",
        "        print(\"\\nIt's a draw!\")\n",
        "        return '-'\n",
        "\n",
        "    def round_V(self):\n",
        "        # After training, this makes action selection random from equally-good choices\n",
        "        for k in self.V.keys():\n",
        "            self.V[k] = round(self.V[k],1)\n",
        "\n",
        "\n",
        "    def __state_values(self, game_states):\n",
        "        return dict((state, self.state_value(state)) for state in game_states)\n",
        "\n",
        "    def __argmax_V(self, state_values):\n",
        "        max_V = max(state_values.values())\n",
        "        chosen_state = random.choice([state for state, v in state_values.items() if v == max_V])\n",
        "        return chosen_state\n",
        "\n",
        "    def __argmin_V(self, state_values):\n",
        "        min_V = min(state_values.values())\n",
        "        chosen_state = random.choice([state for state, v in state_values.items() if v == min_V])\n",
        "        return chosen_state\n",
        "\n",
        "    def __random_V(self, state_values):\n",
        "        return random.choice(list(state_values.keys()))\n",
        "\n",
        "    def __reward(self, game):\n",
        "        # -------------------------------------------------------------------- #\n",
        "        # TODO: implement SPARSE reward for winning and SPARSE penalty for losing\n",
        "        reward = None\n",
        "        if game.winner == self.value_player:\n",
        "            reward = None\n",
        "        elif game.winner:\n",
        "            reward = None\n",
        "        else:\n",
        "            reward = None\n",
        "        return reward\n",
        "        # -------------------------------------------------------------------- #\n",
        "\n",
        "    def __request_human_move(self, game):\n",
        "        allowed_moves = [i+1 for i in range(9) if game.state[i] == ' ']\n",
        "        human_move = None\n",
        "        while not human_move:\n",
        "            idx = int(input('Choose move for {}, from {} : '.format(game.player, allowed_moves)))\n",
        "            if any([i==idx for i in allowed_moves]):\n",
        "                human_move = game.state[:idx-1] + game.player + game.state[idx:]\n",
        "        return human_move\n",
        "\n",
        "def demo_game_stats(agent, games:int=10000):\n",
        "    results = [agent.demo_game() for i in range(games)]\n",
        "    game_stats = {k: results.count(k)/100 for k in ['X', 'O', '-']}\n",
        "    print(\"    percentage results: {}\".format(game_stats))"
      ],
      "metadata": {
        "id": "w2HdutVILl20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make an Agent\n",
        "Note, that in the environemnt defition (above) X always starts.\n",
        "\n",
        "Let's create an Agent, setup some learning parameters, select `O`/`X` symbol for it.\n",
        "\n",
        "Then, evaluate the \"empty\" policy of the agent by playing $10000$ iterations of game. The print out of the efficiency of this AI model will present wins, loses and draws (noted as `-`)."
      ],
      "metadata": {
        "id": "UByT7waCLIIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create you agent (named 'X') and assign the learning rate and discount factor\n",
        "agent = Agent(TicTacToeGame, epsilon = 0.1, alpha = 0.5, value_player='X')\n",
        "\n",
        "print(\"Before learning:\")\n",
        "demo_game_stats(agent)"
      ],
      "metadata": {
        "id": "KV6QSVd1MzEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try it face-2-face in the interactive mode:"
      ],
      "metadata": {
        "id": "L5TylpcTNkKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.interactive_game()"
      ],
      "metadata": {
        "id": "b5WjErc0NotZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the agent\n",
        "\n",
        "You can have the agent play against itself for a number of games to learn how to play the game. In the cell below, you will make the agent play 1000 games. After that, try playing against it again. Can you see any difference?"
      ],
      "metadata": {
        "id": "L4SgR0EhQqgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.learn_game(1000)\n",
        "print(\"After 1000 learning games:\")\n",
        "demo_game_stats(agent)"
      ],
      "metadata": {
        "id": "l6pGQxonQe4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.interactive_game()"
      ],
      "metadata": {
        "id": "-FzAJKDlQ5C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. More training and playing\n",
        "Continue training the method:\n",
        "* First for 4000 more games, then try playing again.\n",
        "* Finally, keep training the model for 10000 games more and try playing again.\n",
        "\n",
        "What do you notice? Is the agent getting harder to beat?"
      ],
      "metadata": {
        "id": "dBtg-a-NQxrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- #\n",
        "# TODO: code goes here\n",
        "\n",
        "# ------------------- #"
      ],
      "metadata": {
        "id": "951ouQlEQ8se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.interactive_game()"
      ],
      "metadata": {
        "id": "Pe2tx4caRBnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Tweak the model\n",
        "Now you can tweak the model, and experiment further:\n",
        "1. Start by changing the AI identity to \"O\" (so that you get to start the games). Re-train and re-try playing.\n",
        "2. Experiment with changing the learning rate and discounting factors.\n",
        "3. Try to plot the improvement proces in function of episodes."
      ],
      "metadata": {
        "id": "3j_ekieBRuPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- #\n",
        "# TODO: experiments goes here\n",
        "\n",
        "# ------------------- #"
      ],
      "metadata": {
        "id": "3T6YrmVvSNSt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}