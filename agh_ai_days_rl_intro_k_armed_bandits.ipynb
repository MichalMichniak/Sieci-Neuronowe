{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalMichniak/Sieci-Neuronowe/blob/main/agh_ai_days_rl_intro_k_armed_bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-armed bandtis\n",
        "\n",
        "In this hands-on part of the workshops we will explore the simplest environment in terms of mathematical formulation: K-armed bandit.\n",
        "\n",
        "Only one state, multiple discrete actions, different rewards distributions.\n",
        "\n",
        "Let's try to solve this problem!\n",
        "\n",
        "<img src=\"https://lh7-eu.googleusercontent.com/KzmEwcpY9we0aaDuKl9pZEpgut4kK3BRxYACxfJTDuXkrjOL36sxBQqWBgAMUn3cScNyVJnJJrlP_rKaUPKt-uuQPDAMv11z1RxuV12i_NMMhrLsp-21Apfvuyp-RaWB0moaBNoH6E1enoe5MxPhVsk\" alt=\"drawing\" width=\"600px\"/>\n",
        "\n",
        "## Credits\n",
        "- This notebook is **heavily based** (i.e. large portions of text and code have been copied) on the work by juanmadlg ( https://github.com/juanmadlg/K-armed-Bandit-Problem/tree/main )\n",
        "- Maciej Aleksandrowicz for AGH AI Days 2024"
      ],
      "metadata": {
        "id": "exgevrXmGseU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages\n",
        "\n",
        "You will use the following packages in this assignment.\n",
        "\n",
        "- [numpy](www.numpy.org) : Fundamental package for scientific computing with Python.\n",
        "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
        "- [tqdm](https://tqdm.github.io/) : A package to display progress bar when running experiments.\n",
        "- [matplotlib](http://matplotlib.org) : Library for plotting graphs in Python.\n",
        "- [scipy](https://scipy.org/) : Library for scientific tools, i.e. statistics.\n"
      ],
      "metadata": {
        "id": "IBw1aHRaNq6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jdc"
      ],
      "metadata": {
        "id": "Lt2BwSojNwsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import jdc\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import norm\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(44)"
      ],
      "metadata": {
        "id": "CGX9Rw4DNzfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The main ideas:\n",
        "\n",
        "1. The Bandit have `k` arms.\n",
        "2. Each of the arm can return different values (sampled from distribution).\n",
        "3. The number of arms and the underlying distributions are constans.\n",
        "4. **We would like to somehow model the Bandit**.\n",
        "\n",
        "\n",
        "## K-Armed Bandit Class\n",
        "\n",
        "Let's introduce a class for holding data and methods related to the Bandit model.\n",
        "\n",
        "These are the elements included in the K-Armed Bandit Class.\n",
        "\n",
        "**Environment defition**:\n",
        "* `k`: The number of arms.\n",
        "* `A`: Vector of size `k`; stores the *mean value of the reward normal distribution* for every arm. In our case it is identical with $Q^*$ function. Initializated each time when the `reset()` is executed.\n",
        "* `arms_mean`: Reference mean value for drawing numbers for the `A` vector.\n",
        "* `standard_deviation`: It is used in two cases:\n",
        "  * To generate the random values in the $A$ vector.\n",
        "  * To generate the random reward of an arm when it is executed.\n",
        "\n",
        "**Values for the decision making policy**:\n",
        "* `N`: Vector of size `k`; counts how many times each lever has been selected.\n",
        "* `Q`: Vector of size `k`; stores the Value-State function $Q$ for each arm (must be trained).\n",
        "* `e`: Epsilon (chance) for selecting a random action.\n",
        "\n",
        "\n",
        "\n",
        "### Importat notes about the notation\n",
        "* The $Q$ function is something which needs to be discovered.\n",
        "* The $Q^*$ is the optimal solution - here, the underlying mathematical model of the rewards.\n",
        "* The $Q$ function is always seen with two parameters: `s` state and `a` action: $Q(s,a)$.\n",
        "  * For this scenario there is **only one** state, so it can be simplified to $Q(a)$. However, when it is important to distinguish such function for particular state, another notation can be used: $V(a)$."
      ],
      "metadata": {
        "id": "db7LCthOOQLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class k_armed_bandit:\n",
        "  def __init__(self, configuration):\n",
        "    '''\n",
        "    Configuration is a dictionary that includes all diferente required settings\n",
        "    Here: {\n",
        "      \"k\": <number_of_arms>,\n",
        "      \"e\": <chance_for_random_action>,\n",
        "      \"arms_mean\": <reference_mean_value_for_reward_initalization>,\n",
        "      \"std_dev\": <standard_deviation_used_for_normal_distributions>\n",
        "      }\n",
        "    '''\n",
        "    # Math model of the k-armed bandit\n",
        "    self.A = None # here, it's identical to the Q* function\n",
        "    self.k = configuration['k']\n",
        "\n",
        "    # Variables for the algorithm\n",
        "    self.N = None # counts lever selections\n",
        "    self.Q = None # estimation of the Q* function\n",
        "    self.e = configuration['e']\n",
        "\n",
        "    # Simplified utility variables\n",
        "    self.arms_reference_mean = configuration['arms_mean']\n",
        "    self.standard_deviation = configuration['std_dev']\n",
        "\n",
        "  def reset(self):\n",
        "    '''\n",
        "    Reset is executed at the begining of each episode. It sets to zero Q and N\n",
        "    and creates a new distribution of Q* (i.e. A) for the k arms.\n",
        "    '''\n",
        "    # ------------------------------------------------------------------------ #\n",
        "    #TODO fill initalize vector Q, N and A (i.e. Q*)\n",
        "    size = (None, ) #set the size\n",
        "    self.Q = None # initalize with zeros\n",
        "    self.N = None # initalize with zeros\n",
        "    self.A = np.random.normal(loc=0.0, scale=0.0, size=(1, ))\n",
        "    # ------------------------------------------------------------------------ #"
      ],
      "metadata": {
        "id": "LQ1uVPTGONtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A tactic for acting\n",
        "\n",
        "We need to do something - select a lever. But:\n",
        "1. Which lever?\n",
        "2. How often to change the selected lever?\n",
        "\n",
        "The thing for selecting an action is called as a ***policy*** - a function which takes the current state and returns an action:\n",
        "$$\n",
        "\\pi_{\\text{policy}}(s_\\text{state}) = a_\\text{action}\n",
        "$$\n",
        "\n",
        "The simplest way to balance *exploration* for information and *exploitation* of the current knowledge is to roll a dice and select between two options:\n",
        "- *be optimal*,\n",
        "- *do something new*.\n",
        "\n",
        "<img src=\"https://resin-expert.com/wp-content/uploads/2020/11/resin-dice.jpg\" alt=\"drawing\" width=\"500px\"/>\n",
        "\n",
        "This method is called:\n",
        "\n",
        "### Episilon-greedy action selection\n",
        "\n",
        "$$\n",
        "\\DeclareMathOperator*{\\argmax}{argmax}\n",
        "a_\\text{action} = \\cases{ \\underset{a}{\\mathrm{argmax}}\\, Q(a) & \\text{with probability $1 - ϵ$}\\\\\n",
        "a \\sim U(\\{a_1, \\dots, a_n\\}) & \\text{with probability $ϵ$}\n",
        "}\n",
        "$$\n",
        "\n",
        "Let's implement these equations in the `k_armed_bandit` class to obtain an action:\n",
        "\n"
      ],
      "metadata": {
        "id": "RxcYraZYSOHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%add_to k_armed_bandit\n",
        "\n",
        "def get_action(self):\n",
        "  '''\n",
        "  €-greedy action selection: exploration vs. exploitation\n",
        "  '''\n",
        "  # -------------------------------------------------------------------------- #\n",
        "  #TODO implement\n",
        "  '''\n",
        "  TODO implmenet, using:\n",
        "  random_action = np.random.randint(<int_value>)\n",
        "  maximum_estimated_value = np.argmax(<vector>)\n",
        "  '''\n",
        "  random_value = np.random.random_sample()\n",
        "\n",
        "  # Exploration, random arm from k-arms\n",
        "\n",
        "  #if ...:\n",
        "  #  return ...\n",
        "\n",
        "  # Exploitation, select the lever with the highest know reward\n",
        "\n",
        "  # return ....\n",
        "  # -------------------------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "RLREv1zZadQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use an action on the Bandit\n",
        "\n",
        "We can generate an action, but we need somehow to use it on the environment (the Bandit).\n",
        "\n",
        "Let's introduce another class method:\n",
        "\n",
        "Return a random value (the reward) based on a normal distribution with $Q^*(\\text{arm})$ (i.e. $A(\\text{arm})$) as mean and the standard deviation."
      ],
      "metadata": {
        "id": "x7PXQZixbKOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%add_to k_armed_bandit\n",
        "\n",
        "def bandit_step(self, action):\n",
        "  '''\n",
        "  Gets value of the selected arm based on a normal distribution with Q*(action) (here: A(action)) as the mean\n",
        "  '''\n",
        "  # -------------------------------------------------------------------------- #\n",
        "  #TODO write the reward function\n",
        "  # reward = np.random.normal(None, None)\n",
        "\n",
        "  # -------------------------------------------------------------------------- #\n",
        "  return reward"
      ],
      "metadata": {
        "id": "spHpBUmPcgt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xs3hQ7uoMizm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Q estimator\n",
        "\n",
        "Now, it's time to train the $Q$ function estimation (i.e. the function which will predict the best return value) with ***ϵ-greedy policy***.\n",
        "\n",
        "For each (time) step of the algorithm:\n",
        "1. Get the action (select an arm) (ϵ-greedy action selection),\n",
        "2. Act on bandit, obtain a $r$ reward for that arm,\n",
        "3. Increment the counter of arm selection,\n",
        "4. Calculate the value function for the arm with the update function:\n",
        "\n",
        "$$\n",
        "Q_{n+1}(a) = Q_n(a) + (r_n - Q_n(a)) / n\n",
        "$$\n",
        "\n",
        "In addition, we would like to log the actions trajectory (the history of actions)."
      ],
      "metadata": {
        "id": "dTYIYklqhTIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%add_to k_armed_bandit\n",
        "\n",
        "def exec(self, steps):\n",
        "  rewards = []\n",
        "  trajectory = []\n",
        "\n",
        "  for i in range(steps):\n",
        "    # ------------------------------------------------------------------------ #\n",
        "    #TODO: Get action & act on the bandit\n",
        "    a = None\n",
        "    r = None\n",
        "    # ------------------------------------------------------------------------ #\n",
        "\n",
        "    self.N[a] += 1\n",
        "    # ------------------------------------------------------------------------ #\n",
        "    #TODO: implement the Update Value Function Q(a)\n",
        "    self.Q[a] = self.Q[a] #...\n",
        "    # ------------------------------------------------------------------------ #\n",
        "\n",
        "    rewards.append(r)\n",
        "    trajectory.append(a)\n",
        "\n",
        "  return rewards, trajectory"
      ],
      "metadata": {
        "id": "BieA7-tshUBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Rewards"
      ],
      "metadata": {
        "id": "WE75e6lwhaXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Scenario\n",
        "\n",
        "* **Arms**: 10 (k)\n",
        "* **Epislon**: 0.1\n",
        "* **Steps**: 800\n",
        "* **Mean**: 1\n",
        "* **Standard deviation**: 1\n",
        "* **Steps**: 800"
      ],
      "metadata": {
        "id": "Ld_fd1nAhehx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------- #\n",
        "#TODO: Enter the values\n",
        "configuration = {\n",
        "    'k': None,\n",
        "    'e': None,\n",
        "    'arms_mean': None,\n",
        "    'std_dev': None\n",
        "}\n",
        "steps = None\n",
        "# ---------------------------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "ngg9q-gDhZaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------- #\n",
        "# TODO: Create the bandit, with configuration and obtain the rewards\n",
        "\n",
        "rewards, trajectory = None, None\n",
        "# ---------------------------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "uj9Vw14_hhRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the rewards, actions and the trained $Q$:"
      ],
      "metadata": {
        "id": "bdv3JCfrhj8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
        "\n",
        "f.set_figheight(10)\n",
        "f.set_figwidth(10)\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# TODO: plot, where x=number of steps, y=rewards\n",
        "# ax1.function(....)\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "ax1.set_ylabel(\"Reward\")\n",
        "ax1.set_title(\"Obtained rewards\")\n",
        "ax1.grid(True)\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# TODO: plot points, where x=step number, y=action in current step\n",
        "# ax2.scatter(....)\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "ax2.set_title(\"Actions trajectory\")\n",
        "ax2.set_ylabel(\"Action\")\n",
        "ax2.grid(True)\n",
        "\n",
        "ax2.set_xlabel(\"Steps\")\n",
        "f.show()"
      ],
      "metadata": {
        "id": "YG76vZBqhlYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Help values for setting up histograms (based on https://stackoverflow.com/questions/30112420/histogram-for-discrete-values-with-matplotlib)\n",
        "\n",
        "d = 1\n",
        "min_value = 0\n",
        "max_value = configuration[\"k\"] - 1\n",
        "left_of_first_bin = min_value - float(d)/2\n",
        "right_of_last_bin = max_value + float(d)/2\n",
        "bins = np.arange(left_of_first_bin, right_of_last_bin + d, d)\n",
        "\n",
        "f, ax = plt.subplots(1, 1)\n",
        "\n",
        "f.set_figheight(7)\n",
        "f.set_figwidth(7)\n",
        "\n",
        "ax.set_xlim([min_value, max_value])\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# TODO: Create a histogram of trajectory\n",
        "# ax.function(....., bins=bins, edgecolor = \"black\")\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "ax.set_xlabel(\"Action number\")\n",
        "ax.set_ylabel(\"Number of selections\")\n",
        "ax.set_title(\"Actions histogram\")\n",
        "ax.grid(True)\n",
        "\n",
        "f.show()"
      ],
      "metadata": {
        "id": "BcyNNNtlmHed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Q with Q*\n",
        "size = bandit_env.k\n",
        "Q = bandit_env.Q\n",
        "Q_optimal = bandit_env.A\n",
        "std_dev = bandit_env.standard_deviation\n",
        "\n",
        "print(f\"The difference between Q* and Q:\\n {Q_optimal - Q}\")\n",
        "\n",
        "data_Q = np.random.normal(Q, np.ones(size) * std_dev, (300, size))\n",
        "data_Q_optimal = np.random.normal(Q_optimal, np.ones(size) * std_dev, (300, size))\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, sharey=True)\n",
        "fig.set_figheight(7)\n",
        "fig.set_figwidth(15)\n",
        "\n",
        "ax[0].violinplot(data_Q, widths=1.5,\n",
        "                 showmeans=True, showmedians=False, showextrema=False)\n",
        "ax[0].set_xlim(1, size)\n",
        "ax[0].set_xticks(np.arange(1, size+1, 1.0))\n",
        "ax[0].set_ylim(min(Q) - 3 * std_dev, max(Q) + 3 * std_dev)\n",
        "ax[0].set_xlabel(\"Arm (action) number\")\n",
        "ax[0].set_ylabel(\"Reward value\")\n",
        "ax[0].set_title(\"Trained Q function's normal distributions per action\")\n",
        "ax[0].grid()\n",
        "\n",
        "ax[1].violinplot(data_Q_optimal, widths=1.5,\n",
        "                 showmeans=True, showmedians=False, showextrema=False)\n",
        "ax[1].set_xlim(1, size)\n",
        "ax[1].set_xticks(np.arange(1, size+1, 1.0))\n",
        "ax[1].set_xlabel(\"Arm (action) number\")\n",
        "ax[1].set_title(\"Bandit Q* function's normal distributions per action\")\n",
        "ax[1].grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MHfUAsEonqtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate different epsilon values\n",
        "\n",
        "**The rewards curve is too noisy** - it's hard to draw a trend line.\n",
        "\n",
        "We cannot evaluate the difference only with a single execution for each epsilon value - it will be too unreliable.\n",
        "\n",
        "To evaluate the different values of the epsilon it is necessary to execute $N$ different runs an calculate the average for each step.\n",
        "\n",
        "\n",
        "Let's define a function for such evaluation:"
      ],
      "metadata": {
        "id": "IZ_GezQcifum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(configuration, epsilon_values, steps, runs):\n",
        "  all_rewards = []\n",
        "\n",
        "  # Iterate for each of the given epsilon value\n",
        "  for epsilon in epsilon_values:\n",
        "    print(f\"> Execution of {runs} runs with the epsilon {epsilon}\")\n",
        "    configuration['e'] = epsilon\n",
        "\n",
        "    bandit_env = k_armed_bandit(configuration)\n",
        "    epislon_rewards = np.zeros((steps, ))\n",
        "\n",
        "    # Let's execute independent runs\n",
        "    for _ in tqdm(range(runs)):\n",
        "      # We reset the environement everytime - there is no sense in logging of the trajectories.\n",
        "\n",
        "      # ---------------------------------------------------------------------- #\n",
        "      # TODO: reset the environement, collect rewards\n",
        "      # bandit_env. ......\n",
        "      # rewards, _ = ......\n",
        "\n",
        "      # ---------------------------------------------------------------------- #\n",
        "\n",
        "      epislon_rewards = np.add(epislon_rewards, rewards)\n",
        "\n",
        "    # Average for each step\n",
        "    all_rewards.append(epislon_rewards / runs)\n",
        "\n",
        "  return all_rewards"
      ],
      "metadata": {
        "id": "pK4VincmifIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's define some epsilon values. The notes are prepared for `[0.01, 0.1, 0.5]` but experiments can be performed with other values (between `0.0` and `1.0`). Set `runs` to $2000$ , other values should be set as in previous configuration."
      ],
      "metadata": {
        "id": "3AabRZ4Q0wNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------- #\n",
        "# TODO: define configuration\n",
        "epsilon_values = []\n",
        "configuration = {\n",
        "    'k': None,\n",
        "    'e': None,\n",
        "    'arms_mean': None,\n",
        "    'std_dev': None\n",
        "}\n",
        "steps = None\n",
        "runs = None\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "rewards = evaluate(configuration, epsilon_values, steps, runs)"
      ],
      "metadata": {
        "id": "oZ85tCBPirtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(range(steps), rewards[0], label=\"ϵ=0.01\")\n",
        "plt.plot(range(steps), rewards[1], label=\"ϵ=0.1\")\n",
        "plt.plot(range(steps), rewards[2], label=\"ϵ=0.5\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Mean Reward\")\n",
        "plt.title(\"Obtained rewards\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PXiEwXz0ivTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions**\n",
        "\n",
        "We can see the different performances for the different epsilon values:\n",
        "\n",
        "* **epsilon = 0.01**: In this case there is very little exploration and a lot of exploitation. This makes the average reward grow very little by little. What would happen if the number of steps is greater than 800?\n",
        "* **epsilon = 0.1**: This seems to be the value of epsilon with better performance. It gets faster a good average reward and it maintains it. There is a good balance between exploration and exploitation for 800 Steps.\n",
        "* **epsilon = 0.5**: In this case, we can see clearly that there is too much exploration. The result is clearly the worst when you explore/exploit the 50% of the time."
      ],
      "metadata": {
        "id": "FpBTafNiirAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate more Steps\n",
        "\n",
        "Let's evaluate the same epsilon values with more steps. Will epsilon 0.01 be better than 0.1 in the long term?"
      ],
      "metadata": {
        "id": "mgtZxMEVi6NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------- #\n",
        "# TODO: copy & paste previous configuration with epislon_values experiment,\n",
        "# set steps to a greater value (for instance: 2000)\n",
        "# epsilon_values = .....\n",
        "# configuration = ....\n",
        "# steps = ......\n",
        "# runs = ......\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "rewards = evaluate(configuration, epsilon_values, steps, runs)"
      ],
      "metadata": {
        "id": "QmrW9mxui5N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(range(steps), rewards[0], label=\"ϵ=0.01\")\n",
        "plt.plot(range(steps), rewards[1], label=\"ϵ=0.1\")\n",
        "plt.plot(range(steps), rewards[2], label=\"ϵ=0.5\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Mean Reward\")\n",
        "plt.title(\"Obtained rewards\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DmY_e960i9De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "Finally, it seems that $0.01$ and $0.1$ epsilon values converge to the same average reward.\n",
        "\n",
        "\n",
        "**Which of them is better? Why?**"
      ],
      "metadata": {
        "id": "jps786tSi-s4"
      }
    }
  ]
}